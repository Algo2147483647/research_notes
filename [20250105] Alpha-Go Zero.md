# [20250105] Alpha-Go Zero

[TOC]

## Introduction

- ***Mastering the Game of Go with Deep Neural Networks and Tree Search***: 提出了一种将蒙特卡罗树搜索与深度神经网络相结合的新方法来解决计算机围棋问题。神经网络通过对人类专家棋局的监督学习和自我对弈的强化学习进行训练，这是计算机程序首次击败人类职业棋手，标志着人工智能在围棋领域的重大突破。
- ***Mastering the game of Go without human knowledge***: 介绍了 AlphaGo Zero，一种完全基于强化学习，无需人类数据、指导或除游戏规则之外的领域知识的算法。AlphaGo Zero 通过自我对弈训练神经网络，以预测自己的落子选择和游戏胜负，从完全零基础开始实现了超越人类的性能，以 100-0 的战绩战胜了此前击败过世界冠军的 AlphaGo。
- ***A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and Go Through Self-Play***: 将 AlphaGo Zero 的方法推广到一个通用的强化学习算法 AlphaZero，使其可以在多种具有挑战性的游戏中实现超越人类的性能。AlphaZero 仅从随机游戏开始，除了游戏规则外不借助任何领域知识，就在国际象棋、将棋和围棋等游戏中击败了世界冠军程序，展示了该算法强大的通用性和学习能力。

## 创新点

- 从零开始，没有任何人类棋谱的先验知识。
- 通过**自我对弈**产生训练数据，并不断训练神经网络。
- 使用**蒙特卡洛树搜索（MCTS）**结合神经网络来选择最佳落子。

## 系统

### Policy-Value Network



网络结构通常为 **ResNet**（残差网络），但支持社区用户自由调整网络大小（如 15 层或 40 层的网络）。



输入层：接收 19×19×17 的图像堆栈作为输入，这一结构专门针对围棋棋盘设计。其中 17 个二进制特征平面包含了丰富的棋局信息，8 个特征平面用于表示当前玩家棋子的位置，8 个用于表示对手棋子的位置，还有 1 个特征平面表示当前下棋的颜色。这些特征平面的设置，使得模型能够全面获取棋盘上的局势，为后续决策提供基础。

输出层：分为策略头和价值头。策略头经过卷积、批量归一化、整流非线性激活函数后，再通过全连接线性层输出大小为 19²+1=362 的向量，对应所有交叉点和 pass move 的 logit 概率，以此来指导下棋的策略选择。价值头同样经过一系列操作，最终通过双曲正切（tanh）非线性激活函数输出一个标量，用于评估当前玩家获胜的概率，反映当前棋局的价值 。

卷积层与残差块：输入数据首先经过一个卷积块，该卷积块由卷积、批量归一化和整流非线性激活函数等模块组成。接着数据会进入由 19 或 39 个残差块构成的残差塔。每个残差块包含卷积、批量归一化、整流非线性激活函数等操作，并且通过跳跃连接将输入与输出相加，这种结构有助于模型更好地学习数据中的特征，提升模型的训练效果和泛化能力。

## Training

### Self-playing

每一局对弈会生成大量的训练样本：

- 状态-动作对：从每一步搜索得到的概率分布。
- 胜负结果：从整局棋的胜负结果回传到所有经过的节点，作为价值头的目标。



在自我对弈中使用蒙特卡洛树搜索（MCTS）进行模拟

通过当前神经网络的策略头预测每一步的概率。

使用 MCTS 执行大量模拟，以决定最优落子。

自我对弈的对局数据包括：

- 棋盘状态。
- 每一步的 MCTS 产生的动作概率分布。
- 最终对局的胜负结果。

### Train neural network

使用自我对弈产生的训练数据，优化神经网络的损失函数。

损失函数包括：

- **策略损失**：预测的动作概率与 MCTS 生成的概率分布之间的差异（交叉熵）。
- **价值损失**：网络估计的胜率与实际对局胜负结果之间的误差（均方误差）。
- **正则化**：防止过拟合的 L2 正则化项。



### 迭代优化

- 每隔一段时间，新训练的网络会与旧版本的网络进行对弈（比赛）。
- 如果新网络胜率超过一定阈值（例如 55%），则用新网络替换旧网络，继续作为主网络进行训练。
- 这个循环过程保证了网络的持续进步。

### Training Analysis

3 天训练阶段：在初始的 3 天训练时间里，AlphaGo Zero 进行了大量自我对弈，生成了 490 万局自对弈游戏。每次蒙特卡罗树搜索（MCTS）使用 1600 次模拟，这意味着每一步棋的思考都基于大量模拟来做出决策。参数更新来自众多小批量数据，每个小批量包含 2048 个位置，这些数据均匀采样自最近的 50 万局自对弈游戏，以此不断优化神经网络参数。



40 天训练阶段：后续扩展训练时，AlphaGo Zero 持续训练约 40 天，在此期间生成了 2900 万局自对弈游戏，训练规模大幅提升。参数更新则是基于 310 万个小批量数据，每个小批量同样包含 2048 个位置，进一步对神经网络参数进行优化，使模型性能不断提升。



上述的自我对弈和神经网络训练不断循环，模型越来越强：

初始时，模型只是随机走子，但随着训练迭代，逐渐学会围棋策略和布局。

**超越人类棋手**：训练仅 3 天后，就击败了 AlphaGo Lee 版本（击败李世石的版本）。

**自我进化**：通过更长时间的训练，AlphaGo Zero 展现出完全不同于人类的围棋风格。



## Leela-Zero

Leela Zero 是一个开源的围棋 AI 项目，Leela Zero 和 AlphaGo Zero 的核心思想一致。它由 Gian-Carlo Pascutto 创建，其目标是完全基于自我对弈来训练出一个从零基础到超越人类顶尖棋手水平的围棋 AI。

- AlphaGo Zero 是由 Google DeepMind 开发的闭源项目。
- Leela Zero 是开源的，由围棋社区共同维护和改进。

[leela-zero/leela-zero: Go engine with no human-provided knowledge, modeled after the AlphaGo Zero paper. (github.com)](https://github.com/leela-zero/leela-zero)





